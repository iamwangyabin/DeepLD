{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "# import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import PIL \n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5578"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "MODEL_PATH = '../models'\n",
    "if MODEL_PATH not in sys.path:\n",
    "    sys.path.append(MODEL_PATH)\n",
    "\n",
    "in_dir='../dataset/color/'\n",
    "img_paths = [x.path for x in os.scandir(in_dir) if x.name.endswith('.jpg') or x.name.endswith('.png')]\n",
    "len(img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 484, 648])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = torchvision.transforms.Compose([\n",
    "#     transforms.Scale(imsize), \n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def image_loader(image_name):\n",
    "    \"\"\"load image, returns cuda tensor\"\"\"\n",
    "    image = PIL.Image.open(image_name)\n",
    "    image = loader(image).float()\n",
    "    image = Variable(image, requires_grad=True)\n",
    "    image = image.unsqueeze(0)  #this is for VGG, may not be needed for ResNet\n",
    "    return image #assumes that you're using GPU\n",
    "\n",
    "image = image_loader(img_paths[0])\n",
    "image=torch.nn.functional.interpolate(image,(image.shape[2]//2, image.shape[3]//2)) # back to original resolution\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import resnet_detector\n",
    "model = resnet_detector.DetectorModel()\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.001)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "score_maps_list,endpoints = model(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build_multi_scale_deep_detector_3DNMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "photos=image\n",
    "batch_size = photos.shape[0]\n",
    "height = photos.shape[2]\n",
    "width = photos.shape[3]\n",
    "C = photos.shape[1]\n",
    "\n",
    "scale_factors = endpoints['scale_factors']\n",
    "scale_factors_tensor=torch.tensor(scale_factors)\n",
    "num_scale = len(score_maps_list)\n",
    "scale_logits = [None] * num_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def instance_normalization(inputs):\n",
    "    # normalize 0-means 1-variance in each sample (not take batch-axis)\n",
    "    inputs_dim = inputs.get_shape().ndims\n",
    "    # Epsilon to be used in the tf.nn.batch_normalization\n",
    "    var_eps = 1e-3    \n",
    "    if inputs_dim == 4:\n",
    "        moments_dims = [1,2] # NHWC format\n",
    "    elif inputs_dim == 2:\n",
    "        moments_dims = [1]\n",
    "    else:\n",
    "        raise ValueError('instance_normalization suppose input dim is 4: inputs_dim={}\\n'.format(inputs_dim))\n",
    "    mean, variance = tf.nn.moments(inputs, axes=moments_dims, keep_dims=True)\n",
    "    outputs = tf.nn.batch_normalization(inputs, mean, variance, None, None, var_eps) # non-parametric normalization\n",
    "    return outputs\n",
    "```\n",
    "================torch.nn.BatchNorm2d(1, eps=1e-06, momentum=None, affine=None, track_running_stats=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_normalization=torch.nn.BatchNorm2d(1, eps=1e-06, momentum=None, affine=None, track_running_stats=None)\n",
    "for i in range(num_scale):\n",
    "    logits = instance_normalization(score_maps_list[i])\n",
    "    logits = torch.nn.functional.interpolate(score_maps_list[i],(height, width)) # back to original resolution\n",
    "    scale_logits[i] = logits\n",
    "scale_logits=torch.cat(scale_logits, dim=1) # [B,S,H,W]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## def soft_nms_3d(scale_logits, ksize, com_strength=1.0):\n",
    "    # apply softmax on scalespace logits\n",
    "    # scale_logits: [B,S,H,W]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_scales =scale_logits.shape[1]\n",
    "ksize=15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[B,S,H,W] -> (B,C,S,H,W)--batch size , channel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 3, 484, 648])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale_logits_d=scale_logits[...,None].permute(0,4,1,2,3)\n",
    "scale_logits_d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxpool3d=torch.nn.MaxPool3d((num_scales, ksize, ksize),stride=(num_scales, 1, 1),padding=(0,(ksize-1)//2,(ksize-1)//2),dilation=1)\n",
    "max_maps=maxpool3d(scale_logits_d)\n",
    "max_maps = max_maps.squeeze(1) # (B,C,S,H,W) -> [B,S,H,W]\n",
    "# max_maps = max_maps.permute() # [B,S,H,W]\n",
    "com_strength=3\n",
    "exp_maps = torch.exp(com_strength * (scale_logits-max_maps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 3, 484, 648])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_maps_d=exp_maps[...,None].permute(0,4,1,2,3)\n",
    "exp_maps_d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv3d=torch.nn.Conv3d(1, 1, (num_scales,15,15), stride=(num_scales, 1, 1), padding=(0,(ksize-1)//2,(ksize-1)//2), dilation=1,  bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 484, 648])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_ex=conv3d(exp_maps_d)\n",
    "sum_ex = sum_ex.squeeze(1)\n",
    "sum_ex.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = exp_maps / (sum_ex + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 484, 648])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## soft_max_and_argmax_1d\n",
    "(inputs, axis=-1, inputs_index=None, keep_dims=False, com_strength1=250.0, com_strength2=250.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_max_and_argmax_1d(inputs, axis=2, inputs_index=None, keep_dims=False, com_strength1=250.0, com_strength2=250.0):\n",
    "    # Safe softmax\n",
    "    inputs_max,_=torch.max(inputs, dim=axis, keepdim=keep_dims, out=None)\n",
    "    inputs_exp1 = torch.exp(com_strength1*(inputs - inputs_max))\n",
    "    inputs_softmax1_max,_=torch.max(inputs_exp1, dim=axis, keepdim=keep_dims, out=None)\n",
    "    inputs_softmax1 = inputs_exp1 / (inputs_softmax1_max + 1e-8)\n",
    "\n",
    "    inputs_exp2 = torch.exp(com_strength2*(inputs - inputs_max))\n",
    "    inputs_softmax2_max,_=torch.max(inputs_exp2, dim=axis, keepdim=keep_dims, out=None)\n",
    "    inputs_softmax2 = inputs_exp2 / (inputs_softmax2_max + 1e-8)\n",
    "\n",
    "    inputs_max,_=torch.max(inputs * inputs_softmax1, dim=axis, keepdim=keep_dims, out=None)\n",
    "    \n",
    "    inputs_index_shp = [1,]*(len(inputs.shape)-1)\n",
    "    inputs_index_shp[axis] = -1\n",
    "    if inputs_index is None:\n",
    "        inputs_index = torch.arange(inputs.shape[axis],dtype=inputs.dtype) # use 0,1,2,..,inputs.shape[axis]-1\n",
    "    print(inputs_index_shp)\n",
    "    inputs_index = inputs_index.view(1,-1,1).squeeze().unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n",
    "    print(inputs_softmax2)\n",
    "    print(inputs_index)\n",
    "\n",
    "    inputs_amax = torch.sum(inputs_index.float() * inputs_softmax2, dim=axis, keepdim=keep_dims)\n",
    "    \n",
    "    return inputs_max, inputs_amax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'FloatTensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-7f3068fdbae0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscale_factors_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'FloatTensor'"
     ]
    }
   ],
   "source": [
    "scale_factors_tensor.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, -1, 1]\n",
      "tensor([[[[2.6528e-03, 1.0000e+00, 2.5700e-18,  ..., 0.0000e+00,\n",
      "           3.3165e-35, 0.0000e+00],\n",
      "          [2.5302e-18, 5.1891e-07, 8.4460e-32,  ..., 3.8302e-40,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 1.0000e+00, 1.5674e-12,  ..., 6.4809e-32,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          ...,\n",
      "          [3.6468e-23, 4.7278e-13, 1.1287e-25,  ..., 3.1145e-01,\n",
      "           1.2950e-03, 9.0395e-07],\n",
      "          [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "           1.4257e-14, 6.0391e-01],\n",
      "          [1.0250e-05, 1.0000e+00, 1.0000e+00,  ..., 1.7135e-03,\n",
      "           9.5242e-06, 5.6488e-06]],\n",
      "\n",
      "         [[1.0000e+00, 6.6787e-01, 1.0000e+00,  ..., 1.0000e+00,\n",
      "           1.0000e+00, 0.0000e+00],\n",
      "          [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 1.0946e-06, 1.0000e+00,  ..., 1.0000e+00,\n",
      "           1.0000e+00, 0.0000e+00],\n",
      "          ...,\n",
      "          [1.0000e+00, 1.3028e-12, 6.7345e-23,  ..., 1.5291e-01,\n",
      "           1.0000e+00, 1.0000e+00],\n",
      "          [1.5227e-16, 5.0236e-11, 6.4508e-17,  ..., 4.4843e-05,\n",
      "           1.9460e-03, 3.3301e-04],\n",
      "          [4.2133e-03, 0.0000e+00, 0.0000e+00,  ..., 1.0000e+00,\n",
      "           4.8326e-18, 1.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 0.0000e+00, 7.4544e-33,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 1.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           1.0000e+00, 1.0000e+00],\n",
      "          [1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 1.0000e+00],\n",
      "          ...,\n",
      "          [6.9155e-24, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "           4.5584e-01, 1.0297e-02],\n",
      "          [2.8625e-17, 4.3161e-15, 1.2505e-17,  ..., 2.9928e-01,\n",
      "           1.0000e+00, 1.0000e+00],\n",
      "          [1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.0002e-04,\n",
      "           1.0000e+00, 3.6582e-07]]]], grad_fn=<DivBackward0>)\n",
      "tensor([[[[1.0000]],\n",
      "\n",
      "         [[0.3536]],\n",
      "\n",
      "         [[0.1250]]]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "max_heatmaps, max_scales = soft_max_and_argmax_1d(probs, axis=1, \n",
    "                                    inputs_index=scale_factors_tensor, keep_dims=False,\n",
    "                                    com_strength1=100,\n",
    "                                    com_strength2=100) # both output = [B,H,W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 484, 648])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_heatmaps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3562, 1.2361, 0.3536,  ..., 0.3536, 0.3536, 0.1250],\n",
       "         [0.3536, 0.3536, 0.3536,  ..., 0.3536, 0.1250, 0.1250],\n",
       "         [0.1250, 1.0000, 0.3536,  ..., 0.3536, 0.3536, 0.1250],\n",
       "         ...,\n",
       "         [0.3536, 0.1250, 0.1250,  ..., 0.4905, 0.4118, 0.3548],\n",
       "         [1.0000, 1.0000, 1.0000,  ..., 1.0374, 0.1257, 0.7290],\n",
       "         [0.1265, 1.0000, 1.0000,  ..., 0.3553, 0.1250, 0.3536]]],\n",
       "       grad_fn=<SumBackward2>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 0.3536, 0.1250], dtype=torch.float64)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale_factors_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max,max_index=torch.max(x, dim=1, keepdim=True, out=None)\n",
    "x-max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1289"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
