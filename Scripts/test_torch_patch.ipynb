{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "# import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import PIL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5578"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "MODEL_PATH = '../models'\n",
    "if MODEL_PATH not in sys.path:\n",
    "    sys.path.append(MODEL_PATH)\n",
    "\n",
    "in_dir='../dataset/color/'\n",
    "img_paths = [x.path for x in os.scandir(in_dir) if x.name.endswith('.jpg') or x.name.endswith('.png')]\n",
    "len(img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = torchvision.transforms.Compose([\n",
    "#     transforms.Scale(imsize), \n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def image_loader(image_name):\n",
    "    \"\"\"load image, returns cuda tensor\"\"\"\n",
    "    image = PIL.Image.open(image_name)\n",
    "    image = loader(image).float()\n",
    "    image = Variable(image, requires_grad=True)\n",
    "    image = image.unsqueeze(0)  #this is for VGG, may not be needed for ResNet\n",
    "    return image #assumes that you're using GPU\n",
    "\n",
    "image = image_loader(img_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import resnet_detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet_detector.DetectorModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(),lr=0.001)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "score_maps_list,endpoints = model(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从网络输出得到patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 121, 162])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_maps_list[8].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 为了得到需要的多尺度特征点，需要在上面基础网络上再封装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build_multi_scale_deep_detector_3DNMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "photos=image\n",
    "batch_size = photos.shape[0]\n",
    "height = photos.shape[2]\n",
    "width = photos.shape[3]\n",
    "C = photos.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_factors = endpoints['scale_factors']\n",
    "scale_factors_tensor=torch.tensor(scale_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 0.7711, 0.5946, 0.4585, 0.3536, 0.2726, 0.2102, 0.1621, 0.1250],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale_factors_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_scale = len(score_maps_list)\n",
    "num_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale_logits = [None] * num_scale\n",
    "scale_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 342, 458])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=score_maps_list[4]\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 342, 1])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.var(x.view(x.shape[0], x.shape[1], x.shape[2], x.shape[3]), dim=3, keepdim=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.BatchNorm2d(1, momentum=None, affine=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.1682, -2.2953, -2.8026,  ..., -4.7189, -1.0219,  0.9656],\n",
       "          [-0.1638, -1.3107, -2.3589,  ..., -3.2995, -1.6894, -1.0583],\n",
       "          [ 2.0597,  0.0909, -1.4558,  ..., -6.4776, -3.1175,  0.0154],\n",
       "          ...,\n",
       "          [-3.1242, -1.5743,  1.5801,  ...,  2.6838,  2.7484,  1.7357],\n",
       "          [-0.6293,  2.4569, -0.7084,  ...,  1.9609,  2.7929,  1.0595],\n",
       "          [-1.8180,  2.7054,  0.7647,  ...,  4.1476,  2.6694,  0.8345]]]],\n",
       "       grad_fn=<NativeBatchNormBackward>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_patch_extraction(config, det_endpoints, photos=None, name='PatchExtract'):\n",
    "    with tf.name_scope(name):\n",
    "        batch_inds = det_endpoints['batch_inds']\n",
    "        kpts = det_endpoints['kpts']\n",
    "        kpts_scale = det_endpoints['kpts_scale']\n",
    "        kpts_ori = det_endpoints['kpts_ori']\n",
    "\n",
    "        if config.desc_inputs == 'det_feats':\n",
    "            feat_maps = tf.identity(det_endpoints['feat_maps']) \n",
    "        elif config.desc_inputs == 'photos':\n",
    "            feat_maps = tf.identity(photos)\n",
    "        elif config.desc_inputs == 'concat':\n",
    "            feat_maps = tf.concat([photos, det_endpoints['feat_maps']], axis=-1)\n",
    "        else:\n",
    "            raise ValueError('Unknown desc_inputs: {}'.format(config.desc_inputs))\n",
    "\n",
    "        patches = transformer_crop(feat_maps, config.patch_size, batch_inds, kpts,\n",
    "                        kpts_scale=kpts_scale, kpts_ori=kpts_ori)\n",
    "\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(top_k):\n",
    "    coords = tf.where(tf.greater(top_k, 0.))\n",
    "    num_kpts = tf.reduce_sum(top_k, axis=[1,2,3])\n",
    "    coords = tf.cast(coords, tf.int32)\n",
    "    batch_inds, kp_y, kp_x, _ = tf.split(coords, 4, axis=-1)\n",
    "    batch_inds = tf.reshape(batch_inds, [-1])\n",
    "    kpts = tf.concat([kp_x, kp_y], axis=1)\n",
    "\n",
    "    num_kpts = tf.cast(num_kpts, tf.int32)\n",
    "    # kpts: [N,2] (N=B*K)\n",
    "    # batch_inds: N,\n",
    "    # num_kpts: B\n",
    "    return kpts, batch_inds, num_kpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_crop(\n",
    "    images, out_size, \n",
    "    batch_inds, \n",
    "    kpts_xy, \n",
    "    kpts_scale=None, \n",
    "    kpts_ori=None, \n",
    "    thetas=None):\n",
    "    # images : [B,C,H,W]\n",
    "    # out_size : (out_width, out_height)\n",
    "    # batch_inds : [B*K,] tf.int32 [0,B)\n",
    "    # kpts_xy : [B*K,2] tf.float32 or whatever\n",
    "    # kpts_scale : [B*K,] tf.float32\n",
    "    # kpts_ori : [B*K,2] tf.float32 (cos,sin)\n",
    "    if isinstance(out_size, int):\n",
    "        out_width = out_height = out_size\n",
    "    else:\n",
    "        out_width, out_height = out_size\n",
    "    hoW = out_width // 2\n",
    "    hoH = out_height // 2\n",
    "\n",
    "    num_batch = images.shape[0]\n",
    "    height = images.shape[2]\n",
    "    width = images.shape[3]\n",
    "    C = images.shape[1]\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-4b18b7861e93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_height\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.59460356, 0.35355339, 0.2102241 , 0.125     ])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_scale=2**-3\n",
    "max_scale=1\n",
    "num_scales=5\n",
    "\n",
    "scale_log_factors = np.linspace(np.log(max_scale), np.log(min_scale), num_scales)\n",
    "scale_factors = np.exp(scale_log_factors)\n",
    "scale_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
