{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "# import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import PIL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5578"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "MODEL_PATH = '../models'\n",
    "if MODEL_PATH not in sys.path:\n",
    "    sys.path.append(MODEL_PATH)\n",
    "\n",
    "in_dir='../dataset/color/'\n",
    "img_paths = [x.path for x in os.scandir(in_dir) if x.name.endswith('.jpg') or x.name.endswith('.png')]\n",
    "len(img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = torchvision.transforms.Compose([\n",
    "#     transforms.Scale(imsize), \n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def image_loader(image_name):\n",
    "    \"\"\"load image, returns cuda tensor\"\"\"\n",
    "    image = PIL.Image.open(image_name)\n",
    "    image = loader(image).float()\n",
    "    image = Variable(image, requires_grad=True)\n",
    "    image = image.unsqueeze(0)  #this is for VGG, may not be needed for ResNet\n",
    "    return image #assumes that you're using GPU\n",
    "\n",
    "image = image_loader(img_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import resnet_detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet_detector.DetectorModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(),lr=0.001)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "score_maps_list,endpoints = model(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从网络输出得到patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 157, 210])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_maps_list[7].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 为了得到需要的多尺度特征点，需要在上面基础网络上再封装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build_multi_scale_deep_detector_3DNMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "photos=image\n",
    "batch_size = photos.shape[0]\n",
    "height = photos.shape[2]\n",
    "width = photos.shape[3]\n",
    "C = photos.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_factors = endpoints['scale_factors']\n",
    "scale_factors_tensor=torch.tensor(scale_factors)\n",
    "num_scale = len(score_maps_list)\n",
    "scale_logits = [None] * num_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def instance_normalization(inputs):\n",
    "    # normalize 0-means 1-variance in each sample (not take batch-axis)\n",
    "    inputs_dim = inputs.get_shape().ndims\n",
    "    # Epsilon to be used in the tf.nn.batch_normalization\n",
    "    var_eps = 1e-3    \n",
    "    if inputs_dim == 4:\n",
    "        moments_dims = [1,2] # NHWC format\n",
    "    elif inputs_dim == 2:\n",
    "        moments_dims = [1]\n",
    "    else:\n",
    "        raise ValueError('instance_normalization suppose input dim is 4: inputs_dim={}\\n'.format(inputs_dim))\n",
    "    mean, variance = tf.nn.moments(inputs, axes=moments_dims, keep_dims=True)\n",
    "    outputs = tf.nn.batch_normalization(inputs, mean, variance, None, None, var_eps) # non-parametric normalization\n",
    "    return outputs\n",
    "```\n",
    "================torch.nn.BatchNorm2d(1, eps=1e-06, momentum=None, affine=None, track_running_stats=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_normalization=torch.nn.BatchNorm2d(1, eps=1e-06, momentum=None, affine=None, track_running_stats=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spatial_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_scale):\n",
    "    logits = instance_normalization(score_maps_list[i])\n",
    "    _heatmaps = spatial_softmax(logits, config.sm_ksize, config.com_strength)\n",
    "    _heatmaps = tf.image.resize_images(_heatmaps, (height, width)) # back to original resolution\n",
    "    multi_scale_heatmaps[i] = _heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 1., 2.],\n",
       "          [3., 4., 5.],\n",
       "          [6., 7., 8.]]]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.arange(0,9.0,1).reshape([1,1,3,3])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.5492, -1.1619, -0.7746],\n",
       "          [-0.3873,  0.0000,  0.3873],\n",
       "          [ 0.7746,  1.1619,  1.5492]]]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ba(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.1248],\n",
       "          [0.0620],\n",
       "          [0.1807],\n",
       "          [0.1100],\n",
       "          [0.0572],\n",
       "          [0.0568],\n",
       "          [0.0593],\n",
       "          [0.0564],\n",
       "          [0.0508],\n",
       "          [0.0610],\n",
       "          [0.0539],\n",
       "          [0.0659],\n",
       "          [0.0690],\n",
       "          [0.0579],\n",
       "          [0.0638],\n",
       "          [0.0942],\n",
       "          [0.0777],\n",
       "          [0.0793],\n",
       "          [0.0909],\n",
       "          [0.0740],\n",
       "          [0.0856],\n",
       "          [0.0851],\n",
       "          [0.0954],\n",
       "          [0.0862],\n",
       "          [0.0875],\n",
       "          [0.0909],\n",
       "          [0.1084],\n",
       "          [0.1036],\n",
       "          [0.0929],\n",
       "          [0.0936],\n",
       "          [0.0904],\n",
       "          [0.0952],\n",
       "          [0.0903],\n",
       "          [0.1181],\n",
       "          [0.0818],\n",
       "          [0.0680],\n",
       "          [0.0813],\n",
       "          [0.0865],\n",
       "          [0.0682],\n",
       "          [0.0800],\n",
       "          [0.0824],\n",
       "          [0.0749],\n",
       "          [0.0912],\n",
       "          [0.0908],\n",
       "          [0.0919],\n",
       "          [0.0908],\n",
       "          [0.0874],\n",
       "          [0.0719],\n",
       "          [0.0751],\n",
       "          [0.0927],\n",
       "          [0.0890],\n",
       "          [0.0801],\n",
       "          [0.0785],\n",
       "          [0.0771],\n",
       "          [0.0816],\n",
       "          [0.0865],\n",
       "          [0.0967],\n",
       "          [0.0802],\n",
       "          [0.0768],\n",
       "          [0.0840],\n",
       "          [0.0946],\n",
       "          [0.0916],\n",
       "          [0.0748],\n",
       "          [0.0931],\n",
       "          [0.0953],\n",
       "          [0.1232],\n",
       "          [0.1026],\n",
       "          [0.0807],\n",
       "          [0.0931],\n",
       "          [0.0812],\n",
       "          [0.0862],\n",
       "          [0.0666],\n",
       "          [0.0813],\n",
       "          [0.0861],\n",
       "          [0.0914],\n",
       "          [0.1083],\n",
       "          [0.0800],\n",
       "          [0.0516],\n",
       "          [0.0617],\n",
       "          [0.0535],\n",
       "          [0.0528],\n",
       "          [0.0581],\n",
       "          [0.0561],\n",
       "          [0.0539],\n",
       "          [0.0610],\n",
       "          [0.0546],\n",
       "          [0.0576],\n",
       "          [0.0601],\n",
       "          [0.0649],\n",
       "          [0.0626],\n",
       "          [0.0512],\n",
       "          [0.0612],\n",
       "          [0.0548],\n",
       "          [0.0515],\n",
       "          [0.0548],\n",
       "          [0.0403],\n",
       "          [0.0560],\n",
       "          [0.0491],\n",
       "          [0.0555],\n",
       "          [0.0604],\n",
       "          [0.0613],\n",
       "          [0.0597],\n",
       "          [0.0673],\n",
       "          [0.0678],\n",
       "          [0.0509],\n",
       "          [0.0617],\n",
       "          [0.0569],\n",
       "          [0.0563],\n",
       "          [0.0528],\n",
       "          [0.0477],\n",
       "          [0.0508],\n",
       "          [0.0442],\n",
       "          [0.0406],\n",
       "          [0.0404],\n",
       "          [0.0424],\n",
       "          [0.0391],\n",
       "          [0.0462],\n",
       "          [0.0593],\n",
       "          [0.0468],\n",
       "          [0.0472],\n",
       "          [0.0391],\n",
       "          [0.0444],\n",
       "          [0.0566],\n",
       "          [0.0491],\n",
       "          [0.0524],\n",
       "          [0.0449],\n",
       "          [0.0390],\n",
       "          [0.0483],\n",
       "          [0.0434],\n",
       "          [0.0638],\n",
       "          [0.0550],\n",
       "          [0.0574],\n",
       "          [0.0583],\n",
       "          [0.0596],\n",
       "          [0.0673],\n",
       "          [0.0497],\n",
       "          [0.0652],\n",
       "          [0.0664],\n",
       "          [0.0759],\n",
       "          [0.0703],\n",
       "          [0.0774],\n",
       "          [0.0629],\n",
       "          [0.0633],\n",
       "          [0.0645],\n",
       "          [0.0532],\n",
       "          [0.0481],\n",
       "          [0.0444],\n",
       "          [0.0514],\n",
       "          [0.0331],\n",
       "          [0.0375],\n",
       "          [0.0379],\n",
       "          [0.0401],\n",
       "          [0.0470],\n",
       "          [0.0726],\n",
       "          [0.1071],\n",
       "          [0.0661],\n",
       "          [0.1527]]]], grad_fn=<VarBackward1>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.var(x.view(1, -1, x.shape[2], x.shape[3]), dim=3, keepdim=True,unbiased=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.BatchNorm2d(1, momentum=None, affine=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.1682, -2.2953, -2.8026,  ..., -4.7189, -1.0219,  0.9656],\n",
       "          [-0.1638, -1.3107, -2.3589,  ..., -3.2995, -1.6894, -1.0583],\n",
       "          [ 2.0597,  0.0909, -1.4558,  ..., -6.4776, -3.1175,  0.0154],\n",
       "          ...,\n",
       "          [-3.1242, -1.5743,  1.5801,  ...,  2.6838,  2.7484,  1.7357],\n",
       "          [-0.6293,  2.4569, -0.7084,  ...,  1.9609,  2.7929,  1.0595],\n",
       "          [-1.8180,  2.7054,  0.7647,  ...,  4.1476,  2.6694,  0.8345]]]],\n",
       "       grad_fn=<NativeBatchNormBackward>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_patch_extraction(config, det_endpoints, photos=None, name='PatchExtract'):\n",
    "    with tf.name_scope(name):\n",
    "        batch_inds = det_endpoints['batch_inds']\n",
    "        kpts = det_endpoints['kpts']\n",
    "        kpts_scale = det_endpoints['kpts_scale']\n",
    "        kpts_ori = det_endpoints['kpts_ori']\n",
    "\n",
    "        if config.desc_inputs == 'det_feats':\n",
    "            feat_maps = tf.identity(det_endpoints['feat_maps']) \n",
    "        elif config.desc_inputs == 'photos':\n",
    "            feat_maps = tf.identity(photos)\n",
    "        elif config.desc_inputs == 'concat':\n",
    "            feat_maps = tf.concat([photos, det_endpoints['feat_maps']], axis=-1)\n",
    "        else:\n",
    "            raise ValueError('Unknown desc_inputs: {}'.format(config.desc_inputs))\n",
    "\n",
    "        patches = transformer_crop(feat_maps, config.patch_size, batch_inds, kpts,\n",
    "                        kpts_scale=kpts_scale, kpts_ori=kpts_ori)\n",
    "\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(top_k):\n",
    "    coords = tf.where(tf.greater(top_k, 0.))\n",
    "    num_kpts = tf.reduce_sum(top_k, axis=[1,2,3])\n",
    "    coords = tf.cast(coords, tf.int32)\n",
    "    batch_inds, kp_y, kp_x, _ = tf.split(coords, 4, axis=-1)\n",
    "    batch_inds = tf.reshape(batch_inds, [-1])\n",
    "    kpts = tf.concat([kp_x, kp_y], axis=1)\n",
    "\n",
    "    num_kpts = tf.cast(num_kpts, tf.int32)\n",
    "    # kpts: [N,2] (N=B*K)\n",
    "    # batch_inds: N,\n",
    "    # num_kpts: B\n",
    "    return kpts, batch_inds, num_kpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_crop(\n",
    "    images, out_size, \n",
    "    batch_inds, \n",
    "    kpts_xy, \n",
    "    kpts_scale=None, \n",
    "    kpts_ori=None, \n",
    "    thetas=None):\n",
    "    # images : [B,C,H,W]\n",
    "    # out_size : (out_width, out_height)\n",
    "    # batch_inds : [B*K,] tf.int32 [0,B)\n",
    "    # kpts_xy : [B*K,2] tf.float32 or whatever\n",
    "    # kpts_scale : [B*K,] tf.float32\n",
    "    # kpts_ori : [B*K,2] tf.float32 (cos,sin)\n",
    "    if isinstance(out_size, int):\n",
    "        out_width = out_height = out_size\n",
    "    else:\n",
    "        out_width, out_height = out_size\n",
    "    hoW = out_width // 2\n",
    "    hoH = out_height // 2\n",
    "\n",
    "    num_batch = images.shape[0]\n",
    "    height = images.shape[2]\n",
    "    width = images.shape[3]\n",
    "    C = images.shape[1]\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.59460356, 0.35355339, 0.2102241 , 0.125     ])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_scale=2**-3\n",
    "max_scale=1\n",
    "num_scales=5\n",
    "\n",
    "scale_log_factors = np.linspace(np.log(max_scale), np.log(min_scale), num_scales)\n",
    "scale_factors = np.exp(scale_log_factors)\n",
    "scale_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
